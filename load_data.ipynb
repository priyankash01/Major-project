{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4e06076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1b1da13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stress1 = pd.read_csv(\n",
    "    \"MindSync_Data/stress_dataset_twitter1.csv\", \n",
    "    sep=';',            # semicolon separator\n",
    "    usecols=[0,1],      # only first 2 columns (Text, Label)\n",
    "    encoding='utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "24aeb9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "stress2 = pd.read_csv(\n",
    "    \"MindSync_Data/stress_dataset_twitter2.csv\", \n",
    "    sep=';', \n",
    "    usecols=[0,1], \n",
    "    encoding='utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbef13e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "92453a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'text', 'example_very_unclear', 'admiration', 'amusement',\n",
      "       'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity',\n",
      "       'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment',\n",
      "       'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love',\n",
      "       'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse',\n",
      "       'sadness', 'surprise', 'neutral'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "goemotions_cols = pd.read_csv(\"MindSync_Data/go_emotions_dataset.csv\", nrows=5)\n",
    "print(goemotions_cols.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "24671eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "goemotions = pd.read_csv(\"MindSync_Data/go_emotions_dataset.csv\", usecols=['text'] + [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',\n",
    "    'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',\n",
    "    'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',\n",
    "    'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization',\n",
    "    'relief', 'remorse', 'sadness', 'surprise', 'neutral'\n",
    "], encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9660f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_df = pd.concat([stress1, stress2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cf086181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label hashtags\n",
      "0   speak-no-evil monkey Can I Be Honest With You...    1.0      NaN\n",
      "1  Frau Goebbels early signs of psychosis psychot...    1.0      NaN\n",
      "2  A lot of work and unfulfilled tasks plunge you...    1.0      NaN\n",
      "3  Private health insurance delivers value for yo...    1.0      NaN\n",
      "4  XpertOnline offers you the convenience of view...    1.0      NaN\n",
      "Index(['text', 'label', 'hashtags'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(stress_df.head())\n",
    "print(stress_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0b8ec306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels: [ 0.  1. nan]\n",
      "                                                text  label hashtags\n",
      "0   speak-no-evil monkey Can I Be Honest With You...      1      NaN\n",
      "1  Frau Goebbels early signs of psychosis psychot...      1      NaN\n",
      "2  A lot of work and unfulfilled tasks plunge you...      1      NaN\n",
      "3  Private health insurance delivers value for yo...      1      NaN\n",
      "4  XpertOnline offers you the convenience of view...      1      NaN\n"
     ]
    }
   ],
   "source": [
    "# Encode 'label' column (convert text labels to numbers)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "stress_df['label'] = le.fit_transform(stress_df['label'])\n",
    "\n",
    "print(\"Encoded labels:\", le.classes_)\n",
    "print(stress_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4748627e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'label', 'hashtags'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(stress_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fa5ce5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agar column name 'text' hai\n",
    "stress_df['text'] = stress_df['text'].str.lower()\n",
    "stress_df['text'] = stress_df['text'].str.replace(r'http\\S+|www\\S+','', regex=True)\n",
    "stress_df['text'] = stress_df['text'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "stress_df['text'] = stress_df['text'].str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "23f3142f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label hashtags\n",
      "0  speaknoevil monkey can i be honest with you gl...      1      NaN\n",
      "1  frau goebbels early signs of psychosis psychot...      1      NaN\n",
      "2  a lot of work and unfulfilled tasks plunge you...      1      NaN\n",
      "3  private health insurance delivers value for yo...      1      NaN\n",
      "4  xpertonline offers you the convenience of view...      1      NaN\n"
     ]
    }
   ],
   "source": [
    "# Lowercase\n",
    "stress_df['text'] = stress_df['text'].str.lower()\n",
    "\n",
    "# Remove URLs\n",
    "stress_df['text'] = stress_df['text'].str.replace(r'http\\S+|www\\S+','', regex=True)\n",
    "\n",
    "# Remove punctuation\n",
    "stress_df['text'] = stress_df['text'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "# Strip extra spaces\n",
    "stress_df['text'] = stress_df['text'].str.strip()\n",
    "\n",
    "# Check\n",
    "print(stress_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d60fe95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  admiration  amusement  \\\n",
      "0                                     that game hurt           0          0   \n",
      "1  sexuality shouldnt be a grouping category it m...           0          0   \n",
      "2         you do right if you dont care then fuck em           0          0   \n",
      "3                                  man i love reddit           0          0   \n",
      "4    name was nowhere near them he was by the falcon           0          0   \n",
      "\n",
      "   anger  annoyance  approval  caring  confusion  curiosity  desire  ...  \\\n",
      "0      0          0         0       0          0          0       0  ...   \n",
      "1      0          0         0       0          0          0       0  ...   \n",
      "2      0          0         0       0          0          0       0  ...   \n",
      "3      0          0         0       0          0          0       0  ...   \n",
      "4      0          0         0       0          0          0       0  ...   \n",
      "\n",
      "   love  nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
      "0     0            0         0      0            0       0        0        1   \n",
      "1     0            0         0      0            0       0        0        0   \n",
      "2     0            0         0      0            0       0        0        0   \n",
      "3     1            0         0      0            0       0        0        0   \n",
      "4     0            0         0      0            0       0        0        0   \n",
      "\n",
      "   surprise  neutral  \n",
      "0         0        0  \n",
      "1         0        0  \n",
      "2         0        1  \n",
      "3         0        0  \n",
      "4         0        1  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "# Lowercase\n",
    "goemotions['text'] = goemotions['text'].str.lower()\n",
    "\n",
    "# Remove URLs\n",
    "goemotions['text'] = goemotions['text'].str.replace(r'http\\S+|www\\S+','', regex=True)\n",
    "\n",
    "# Remove punctuation\n",
    "goemotions['text'] = goemotions['text'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "# Strip extra spaces\n",
    "goemotions['text'] = goemotions['text'].str.strip()\n",
    "\n",
    "# Check\n",
    "print(goemotions.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0843e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Stress dataset (binary classification)\n",
    "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(\n",
    "    stress_df['text'], stress_df['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# GoEmotions dataset (multi-label classification)\n",
    "X_train_g, X_test_g, y_train_g, y_test_g = train_test_split(\n",
    "    goemotions['text'], goemotions.drop(columns=['text']), test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4cce009a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch -q\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# BERT tokenizer load karo\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "71873034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 surprising symptoms that indicate youre more stressed out than you think health stress\n",
      "{'input_ids': tensor([[  101,  1023, 11341,  8030,  2008,  5769,  2115,  2063,  2062, 13233,\n",
      "          2041,  2084,  2017,  2228,  2740,  6911,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "sample = X_train_s.iloc[0]\n",
    "encoded = tokenizer(\n",
    "    sample,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(sample)\n",
    "print(encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9e0a78a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=128):\n",
    "        self.texts = texts.tolist()\n",
    "        self.labels = labels.values if labels is not None else None\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        if self.labels is not None:\n",
    "            label = torch.tensor(self.labels[idx], dtype=torch.float if len(self.labels.shape) > 1 else torch.long)\n",
    "            item['labels'] = label\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7be9598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stress dataset\n",
    "train_dataset_s = TextDataset(X_train_s, y_train_s, tokenizer)\n",
    "test_dataset_s = TextDataset(X_test_s, y_test_s, tokenizer)\n",
    "\n",
    "train_loader_s = DataLoader(train_dataset_s, batch_size=16, shuffle=True)\n",
    "test_loader_s = DataLoader(test_dataset_s, batch_size=16)\n",
    "\n",
    "# GoEmotions dataset\n",
    "train_dataset_g = TextDataset(X_train_g, y_train_g, tokenizer)\n",
    "test_dataset_g = TextDataset(X_test_g, y_test_g, tokenizer)\n",
    "\n",
    "train_loader_g = DataLoader(train_dataset_g, batch_size=16, shuffle=True)\n",
    "test_loader_g = DataLoader(test_dataset_g, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11253639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "896a7a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 1.5.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Check pandas version\n",
    "print(\"Pandas version:\", pd.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a508c5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label hashtags\n",
      "0   speak-no-evil monkey Can I Be Honest With You...    1.0      NaN\n",
      "1  Frau Goebbels early signs of psychosis psychot...    1.0      NaN\n",
      "2  A lot of work and unfulfilled tasks plunge you...    1.0      NaN\n",
      "3  Private health insurance delivers value for yo...    1.0      NaN\n",
      "4  XpertOnline offers you the convenience of view...    1.0      NaN\n",
      "Stress dataset shape: (10951, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load stress dataset 1\n",
    "stress1 = pd.read_csv(\"MindSync_Data/stress_dataset_twitter1.csv\", sep=';', usecols=[0,1], encoding='utf-8')\n",
    "\n",
    "# Load stress dataset 2\n",
    "stress2 = pd.read_csv(\"MindSync_Data/stress_dataset_twitter2.csv\", sep=';', usecols=[0,1], encoding='utf-8')\n",
    "\n",
    "# Combine both stress datasets\n",
    "stress_df = pd.concat([stress1, stress2], ignore_index=True)\n",
    "\n",
    "# Check first few rows\n",
    "print(stress_df.head())\n",
    "print(\"Stress dataset shape:\", stress_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8a475fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  admiration  amusement  \\\n",
      "0                                    That game hurt.           0          0   \n",
      "1   >sexuality shouldn’t be a grouping category I...           0          0   \n",
      "2     You do right, if you don't care then fuck 'em!           0          0   \n",
      "3                                 Man I love reddit.           0          0   \n",
      "4  [NAME] was nowhere near them, he was by the Fa...           0          0   \n",
      "\n",
      "   anger  annoyance  approval  caring  confusion  curiosity  desire  ...  \\\n",
      "0      0          0         0       0          0          0       0  ...   \n",
      "1      0          0         0       0          0          0       0  ...   \n",
      "2      0          0         0       0          0          0       0  ...   \n",
      "3      0          0         0       0          0          0       0  ...   \n",
      "4      0          0         0       0          0          0       0  ...   \n",
      "\n",
      "   love  nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
      "0     0            0         0      0            0       0        0        1   \n",
      "1     0            0         0      0            0       0        0        0   \n",
      "2     0            0         0      0            0       0        0        0   \n",
      "3     1            0         0      0            0       0        0        0   \n",
      "4     0            0         0      0            0       0        0        0   \n",
      "\n",
      "   surprise  neutral  \n",
      "0         0        0  \n",
      "1         0        0  \n",
      "2         0        1  \n",
      "3         0        0  \n",
      "4         0        1  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "GoEmotions shape: (211225, 29)\n"
     ]
    }
   ],
   "source": [
    "goemotions = pd.read_csv(\n",
    "    \"MindSync_Data/go_emotions_dataset.csv\",\n",
    "    usecols=['text', 'admiration', 'amusement', 'anger', 'annoyance', 'approval',\n",
    "             'caring', 'confusion', 'curiosity', 'desire', 'disappointment',\n",
    "             'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear',\n",
    "             'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism',\n",
    "             'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral'],\n",
    "    encoding='utf-8'\n",
    ")\n",
    "\n",
    "print(goemotions.head())\n",
    "print(\"GoEmotions shape:\", goemotions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "813d2f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Text cleaning done!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  # remove URLs\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)        # remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()   # remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "stress_df.columns = ['text', 'label', 'hashtags']\n",
    "stress_df['text'] = stress_df['text'].apply(clean_text)\n",
    "goemotions['text'] = goemotions['text'].apply(clean_text)\n",
    "\n",
    "\n",
    "print(\"✅ Text cleaning done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "319c2395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'label', 'hashtags'], dtype='object')\n",
      "                                                text  label hashtags\n",
      "0  speaknoevil monkey can i be honest with you gl...    1.0      NaN\n",
      "1  frau goebbels early signs of psychosis psychot...    1.0      NaN\n",
      "2  a lot of work and unfulfilled tasks plunge you...    1.0      NaN\n",
      "3  private health insurance delivers value for yo...    1.0      NaN\n",
      "4  xpertonline offers you the convenience of view...    1.0      NaN\n"
     ]
    }
   ],
   "source": [
    "print(stress_df.columns)\n",
    "print(stress_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "00294afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned files saved successfully ✅\n"
     ]
    }
   ],
   "source": [
    "stress_df.to_csv(\"MindSync_Data/stress_clean.csv\", index=False)\n",
    "goemotions.to_csv(\"MindSync_Data/goemotions_clean.csv\", index=False)\n",
    "print(\"Cleaned files saved successfully ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc8d61d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ff28d311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'label', 'hashtags'], dtype='object')\n",
      "Index(['text', 'admiration', 'amusement', 'anger', 'annoyance', 'approval',\n",
      "       'caring', 'confusion', 'curiosity', 'desire', 'disappointment',\n",
      "       'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear',\n",
      "       'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride',\n",
      "       'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(stress_df.columns)\n",
    "print(goemotions.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "02a12c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert GoEmotions one-hot to single label\n",
    "goemotions['label'] = goemotions.drop('text', axis=1).idxmax(axis=1)\n",
    "goemotions = goemotions[['text', 'label']]  # keep only necessary columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4c597af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([stress_df[['text', 'label']], goemotions], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bab96876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset shape: (222176, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bad news dude youre almost 100 years past that</td>\n",
       "      <td>disappointment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no it makes it better</td>\n",
       "      <td>disapproval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>except that teenagers do give half a damn abou...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whats not true exactly that population differe...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>just report his comments let the mods take car...</td>\n",
       "      <td>approval</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text           label\n",
       "0     bad news dude youre almost 100 years past that  disappointment\n",
       "1                              no it makes it better     disapproval\n",
       "2  except that teenagers do give half a damn abou...         neutral\n",
       "3  whats not true exactly that population differe...         neutral\n",
       "4  just report his comments let the mods take car...        approval"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Show result\n",
    "print(\"Combined dataset shape:\", combined_df.shape)\n",
    "combined_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe001c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "482e5db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral           55298\n",
       "admiration        20542\n",
       "approval          15530\n",
       "annoyance         11929\n",
       "disapproval        8917\n",
       "amusement          8862\n",
       "gratitude          8437\n",
       "anger              7956\n",
       "curiosity          7707\n",
       "disappointment     6769\n",
       "confusion          6600\n",
       "love               5310\n",
       "caring             5147\n",
       "realization        5125\n",
       "joy                5120\n",
       "optimism           4994\n",
       "excitement         4375\n",
       "sadness            3863\n",
       "surprise           3472\n",
       "disgust            3420\n",
       "desire             3002\n",
       "fear               2514\n",
       "embarrassment      1720\n",
       "remorse            1648\n",
       "1.0                1268\n",
       "nervousness         946\n",
       "relief              814\n",
       "0.0                 783\n",
       "pride               714\n",
       "grief               494\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df['label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "08186a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of valid emotion labels from your dataset\n",
    "valid_labels = [\n",
    "    'neutral', 'admiration', 'approval', 'annoyance', 'disapproval',\n",
    "    'amusement', 'gratitude', 'anger', 'curiosity', 'disappointment',\n",
    "    'confusion', 'love', 'caring', 'realization', 'joy', 'optimism',\n",
    "    'excitement', 'sadness', 'surprise', 'disgust', 'desire',\n",
    "    'fear', 'embarrassment', 'remorse', 'relief', 'pride', 'grief'\n",
    "]\n",
    "\n",
    "# Keep only these labels\n",
    "combined_df = combined_df[combined_df['label'].isin(valid_labels)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a59f5411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 168223\n",
      "Test set size: 42056\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    combined_df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=combined_df['label']  # safe now\n",
    ")\n",
    "\n",
    "print(\"Training set size:\", len(train_df))\n",
    "print(\"Test set size:\", len(test_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b8286394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /Users/priyanka/anaconda3/lib/python3.11/site-packages (4.29.2)\n",
      "Requirement already satisfied: datasets in /Users/priyanka/anaconda3/lib/python3.11/site-packages (2.12.0)\n",
      "Requirement already satisfied: torch in /Users/priyanka/anaconda3/lib/python3.11/site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from datasets) (2023.4.0)\n",
      "Requirement already satisfied: aiohttp in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: responses<0.19 in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: typing-extensions in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: six in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from responses<0.19->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/priyanka/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "14c9454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9c01a071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1045, 2572, 3110, 3407, 2651, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Example tokenization\n",
    "example = tokenizer(\"I am feeling happy today!\", padding='max_length', truncation=True, max_length=64)\n",
    "print(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3fa5c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=64):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3ec12ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_labels = le.fit_transform(train_df['label'])\n",
    "test_labels = le.transform(test_df['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c8e0bc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EmotionDataset(\n",
    "    texts=train_df['text'].tolist(),\n",
    "    labels=train_labels,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=64\n",
    ")\n",
    "\n",
    "test_dataset = EmotionDataset(\n",
    "    texts=test_df['text'].tolist(),\n",
    "    labels=test_labels,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=64\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8e4198a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6b4b2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, AdamW\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "158cd009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=27, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of emotion classes\n",
    "num_labels = len(le.classes_)\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2c6ea6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "epochs = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c2bd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- Resume logic ---\n",
    "start_epoch = 0\n",
    "if os.path.exists(\"model_epoch_2.pt\"):  # agar checkpoint file exist karti hai\n",
    "    model.load_state_dict(torch.load(\"model_epoch_2.pt\"))  # model weights load\n",
    "    optimizer.load_state_dict(torch.load(\"optimizer_epoch_2.pt\"))  # optimizer state load\n",
    "    start_epoch = 2  # next epoch se resume\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # --- Save checkpoint at end of each epoch ---\n",
    "    torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pt\")\n",
    "    torch.save(optimizer.state_dict(), f\"optimizer_epoch_{epoch+1}.pt\")\n",
    "    print(f\"Checkpoint saved for epoch {epoch+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfd0eb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[0;32m----> 3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      4\u001b[0m predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        \n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Print metrics\n",
    "print(classification_report(true_labels, predictions, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8861a132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af718cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789942f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
